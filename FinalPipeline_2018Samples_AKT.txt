Pipeline Code 2023 Re-do (with newest version of genome):

Batch job: 
#!/bin/bash
#PBS -l nodes=1:ppn=20
#PBS -l walltime=24:00:00
#PBS -N Trim_and_QualityFilter
#PBS -o stdout.txt
#PBS -e stderror.txt
#PBS -q workq
#PBS -A loni_assmig5

cd /work/akt/aktgwas/Bio2023/AllRawReads

###########################################################################

#STEP 1: Trim and quality filtering all files in directory with a loop:
module load bwa/0.7.17/intel-19.0.5 
module load samtools/1.10/intel-19.0.5 

for i in *1.fq.gz
do
/work/akt/Programs/bbmap/bbduk.sh in1=$i in2=${i/1.fq.gz/2.fq.gz} out1=${i/1.fq.gz/clean1}.fq.gz out2=${i/1.fq.gz/}clean2.fq.gz ref=/work/akt/Programs/bbmap/resources/nextera_LMP_adapter.fa.gz ktrim=r tbo tpe qtrim=rl trimq=10
done

############################################################################

#STEP 2: Align to reference genome and convert to sam files:
module load bwa/0.7.17/intel-19.0.5
module load samtools/1.10/intel-19.0.5

#index the genome- make sure genome is in correct folder- putting a path in won't work!
bwa index Mimulus_guttatus_var_IM62.mainGenome.fasta

for i in *_clean1.fq.gz
do
bwa mem -t 20 Mimulus_guttatus_var_IM62.mainGenome.fasta $i ${i/_clean1.fq.gz/_1.fq.gz.fq.gz} > ${i/_clean1.fq.gz/.sam}
done

####Run sam to bam part separate####

for i in *.sam
do
samtools view -S -b ${i} > ${i/.sam/.bam}
done

##########################################################################

#Step 3: Process .bam files (aka polish) by adding read groups, marking duplicates, sorting, and indexing

for i in *bam
do
java -Xmx100G -jar /home/akt/Programs/picard.jar AddOrReplaceReadGroups \
I=$i \
O=${i/bam/rg.bam} \
RGID=H7GWYCCX2_L5 \ <---Change as needed
RGLB=lib1 \  <---Change as needed
RGPL=ILLUMINA \
RGPU=CKDL200146708 \  <---Change as needed
RGSM=$i \
SORT_ORDER=coordinate
done

####Run RG and Dups parts separate####

for i in *rg.bam
do
java -Xmx100G -jar /home/akt/Programs/picard.jar MarkDuplicates \
I=$i \
O=${i/rg.bam/dups.bam} \
METRICS_FILE=${i/rg.bam/final.bam} \
CREATE_INDEX=true
done

################ 

When these steps are complete you can move *.dups.bam and *.dups.bai files to a permanent storage place. 

##################################################################

Step 4: Calling Variants: 

####Calculate Genotype Liklihoods:

#!/bin/bash
#PBS -l nodes=1:ppn=20
#PBS -l walltime=24:00:00
#PBS -N Geno_Likeli-1
#PBS -o stdout.txt
#PBS -e stderror.txt
#PBS -q workq
#PBS -A loni_assmig3

cd /work/akt/aktgwas/Bio2022/AllFiles/BamFinal

/work/akt/Programs/bcftools-1.12/bin/bcftools mpileup -r scaffold_1 -f Mguttatus_256_v2.0.fa *.dups.bam --threads 20 > chr1.vcf

####SNP Calling:

/work/akt/Programs/bcftools-1.12/bin/bcftools call chr1.vcf -v -m -o mguttatus_chr1.vcf --threads 20


#################################################################

Step 5: Filter variants to create polished SNP dataset:

####Index genome for GATK format:

/work/akt/Programs/java -jar picard.jar CreateSequenceDictionary R= Mguttatus_256_v2.0.fa O= Mguttatus_256_v2.0.dict

#Note: Did not need to do this this time as I already have this file, and the accompanying Mguttatus_256_v2_0.fa.fai file!

Select variants (SNPs):

/work/akt/Programs/gatk-4.1.8.1/gatk SelectVariants -R Mguttatus_256_v2.0.fa -V mguttatus_chr1.vcf --select-type-to-include SNP -O chr1_SNP.vcf

***Need to run this over and over for all 14 chromosomes!***

#############################

Quality Filter:

/work/akt/Programs/gatk-4.1.8.1/gatk VariantFiltration -R Mguttatus_256_v2.0.fa -V chr1_SNP.vcf --filter-expression "QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" --filter-name "my_snp_filter" -O chr1_SNP_filtered.vcf

**Need to run this over and over for all 14 chromosomes!***

#############################
#############################
##Do not use this code, use below vcf filtering code instead!!!!!!

##((Filter based on SNP attributes:

/work/akt/Programs/vcftools/bin/vcftools --vcf chr1_SNP_filtered.vcf --max-missing 0.75 --remove-filtered-all --minQ 500 --recode --recode-INFO-all --min-alleles 2 --max-alleles 2 --out chr1_SNP_final_filtered.vcf

**Need to run this over and over for all 14 chromosomes!***))
###This step is outdated, use below instead!

*******######

Try an alternate filtering strategy: ***Definetly need to do this! It is now an expected part of filtering!***

/work/akt/Programs/vcftools/bin/vcftools --vcf chr1_SNP_filtered.vcf --max-missing 0.75 --maf 0.05 --remove-filtered-all --minQ 500 --recode --recode-INFO-all --min-alleles 2 --max-alleles 2 --out chr1_SNP_final_filtered_maf.vcf

Need to run for all 14 chromosomes.
This code works fine- filters out ~150,000 more sites per chromosome.

Note to self: Ways I filtered that I didn't include the first time I ran this: max-missing & maf.

*******######

#############################

Rename file to shorter name:

cp chr1_SNP_final_filtered_.vcf.recode.vcf chr1_SNP_final_filtered.vcf

#############################

Prune based on Linkage Disequilibrium using Plink:

#!/bin/bash
#PBS -l nodes=1:ppn=20
#PBS -l walltime=24:00:00
#PBS -N LDPruningChr1
#PBS -o stdout.txt
#PBS -e stderror.txt
#PBS -q workq
#PBS -A loni_assmig3

cd /work/akt/aktgwas/Bio2022/AllFiles/BamFinal/plinking

First, prune SNP's for LD:

/work/akt/Programs/plink --vcf chr1_SNP_final_filtered.vcf --double-id --allow-extra-chr \
--set-missing-var-ids @:# \
--indep-pairwise 50 10 0.1 --out mimulusprunedchr1

Next, make previous output a text file and separate the columns by colon (:):
sed 's/:/\t/g' mimulusprunedchr1.prune.in > prunelistchr1.txt

/work/akt/Programs/vcftools/bin/vcftools --vcf chr1_SNP_final_filtered.vcf --positions prunelistchr1.txt --recode --recode-INFO-all --out vcf_prunedchr1.vcf

cp vcf_prunedchr1.vcf.recode.vcf vcf_prunedchr1.vcf

**Need to run this over and over for all 14 chromosomes!***

#################################################################################################################
This is the end of pipeline- next is calculating pop gen statistics. 





















